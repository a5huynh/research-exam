\documentclass[10pt,twocolumn]{article}

\usepackage{graphics}
\usepackage{color}

% Set it up so that cvites link to their respective citation in the
% bibliography. Also use sane color instead of green.
\usepackage[colorlinks,citecolor=blue]{hyper ref}

% Setup fonts to use
\usepackage{palatino}
%\usepackage[sfdefault]{quattrocento}

% Setup page geometry
\usepackage[margin=2cm]{geometry}

\begin{document}

\title{
	Crowdsourcing and Its Applications in Cultural Heritage}

\author{
	Andrew Huynh\\
	University of California, San Diego\\
	Department of Computer Science and Engineering\\
	\texttt{a5huynh@cs.ucsd.edu}}

\date{August 20, 2013}

\maketitle

\begin{abstract}
In the past decade, ``human computation'' and ``crowdsourcing'' has emerged as 
a new field of research leveraging the vast human connectivity offered by 
the Internet to solve problems that are often too large for individuals or 
too challenging for current automatic methods. While the physical preservation 
of cultural heritage has been an on-going process for decades, digital 
preservation techniques have only recently been applied to the protection, 
documentation, and interpretation of the world's cultural heritage. 
Crowdsourcing offers a unique way to involve the beneficiaries of the heritage 
being preserved in the process of the actual preservation. We present several 
examples of how crowdsourcing techniques is being utilized to protect, 
document, and understand cultural heritage. Finally, we discuss the 
implications of crowdsourcing on the future of cultural heritage preservation.
\end{abstract}

\section{Introduction}
The rise of "human computation"~\cite{VonAhn2009} and 
"crowdsourcing"~\cite{Howe2006} as a field of research emerged from attempts to 
harness and motivate individuals to finish small tasks by either creating 
enjoyable experiences~\cite{VonAhn2008}, appealing to scientific 
altruism~\cite{Cooper2010}, payment~\cite{Kittur2008}, or related services in 
exchange for participation~\cite{Hull2006}. 

The term crowdsourcing was first coined in 2006 by Jeff Howe in a Wired 
magazine article~\cite{Howe2006}, referring to the concept as a method
and business model~\cite{Howe2008} of outsourcing tasks to the crowd. This 
outsourcing attempts to harness the power of a distributed network of cheap 
labor; people who are motivated to use their spare time to finish small tasks 
traditionally performed by an employee. In recent years, the small tasks outsourced 
have become increasingly complex, relying more and more on human intelligence 
to discover a solution.

Related to, but not synonymous with crowdsourcing is the concept of human 
computation. Human computation, a term used as early as in 1838~\cite{Quinn2011}
and more recently and appropriately in Luis Von Ahn's Ph.D.\ dissertation, referring to the 
idea of using human cognitive abilities to ``solve problems that
computers cannot yet solve~\cite{VonAhn2009}.'' These human ``computations'' can 
refer to not only to numerical computations but those that require creativity, 
intuition, pattern recognition, and other forms of human cognitive processing. 
Thus, while human computation does not necessitate the use of crowdsourcing, it 
is often paired together in order to accomplish the end goal.

Human computation systems using crowdsourcing have been applied to tackle enormous 
problems in multiple fields ranging from image annotation~\cite{VonAhn2004}, galaxy 
classification~\cite{Lintott2008}, music annotation~\cite{Turnbull2007},
text transcription~\cite{VonAhn2008}, protein folding~\cite{Cooper2010}, 
and remote damage assessment for disaster response~\cite{Barrington2012}. They
have systematically demonstrated that reliable data can be collected in large 
amounts through incremental contributions from huge numbers of participants.

Growing in parallel with the rise in data collection 
capabilities~\cite{Crane2002}, digital cultural heritage refers to a variety of 
activities from the development of digital technologies used in exhibits to the 
collection~\cite{Proctor2010}, preservation~\cite{Chrons2011}, and 
interpretation of cultural heritage objects and locations from around the 
world. Over the years, human computation and crowdsourcing have been incorporated
in more and more cultural heritage projects. One of its greatest successes, 
the digitization and transcription of ancient texts~\cite{Owens2013,Chrons2011,VonAhn2008a}
have led to the preservation and digitization of numerous important historical 
texts, including the Dead Sea 
Scrolls~\footnote{http://www.google.com/culturalinstitute/about/deadseascroll/}
, ancient Greek scrolls~\footnote{http://ancientlives.org/}, and Civil War 
Diaries~\cite{Owens2013}. The very essence of cultural heritage invites
public dissemination and accessibility, allowing the masses to explore
and connect with the past. Indeed, the locality of cultural heritage and 
the impact it may have on the world presents unique human computation tasks wherein 
the beneficiaries of the tasks may often be the participants themselves.

To understand how crowdsourcing and human computation has become the field 
it is today and its applications to cultural heritage, we start by 
examining the different methods of crowdsourcing and how they distribute 
small tasks that benefit from human involvement (Section~\ref{sec:active} 
\& \ref{sec:passive}). We can then examine how different approaches, ranging 
from simple clustering techniques to machine learning, can be applied to the 
crowdsourced data to extract insight (Section~\ref{sec:learning}). %Following the discussion of those techniques, we 
%examine how human involvement in those techniques through active feedback or 
%even replacing components to the workflow can lead to even greater insight 
%(Section~\ref{sec:the-crowd-machine}). 
Finally, we examine how cultural heritage has benefited from the distributed human 
involvement of crowdsourcing and human computation 
(Section~\ref{sec:applications-heritage}).

\section{Active Crowdsourcing}
\label{sec:active}

Traditional, ``active'', crowdsourcing techniques first described by Howe in 
2006 rely on a platform in which certain tasks can be requested for completion 
by the public~\cite{Howe2006}. This platform can be represented as a singular 
website dedicated to a specific task, such as galaxy classification in 
GalaxyZoo~\cite{Lintott2008}, or a larger market place in which many different 
types of tasks can be requested, such as the human intelligence tasks (HITs) 
in the Amazon Mechanical Turk~\footnote{http://www.mturk.com/mturk/} marketplace.

At a very fundamental level, these different crowdsourcing platforms must adhere to a 
set of principles that enable participants to finish a task in a meaningful manner 
and accomplish the overarching goals of the platform. Specifically, a platform must
be able to engage and attract participants, design tasks in such a 
way that participants can efficiently complete the task, and include constructs 
to ensure a high-quality of data output.

\subsection{Crowd Engagement}
\label{sec:engagement}

Engaging a large pool of labor and presenting compelling motivation to complete
tasks is essential to a platform's success. Here we examine three different
types of engagement: ``Value Incentives", ``The Greater Good", and ``Games With A 
Purpose."

\subsubsection*{Value Incentive}

Participants who can be engaged and motivated through \textit{Value Incentives} 
expect compensation that has a tangible value, which may come in one of two
forms:

\begin{itemize}
	\item
		\textbf{Cold Hard Cash} - Marketplaces such as Amazon Mechanical Turk
		present a financial reward in the exchange for completion of tasks.
		This financial reward is set by the task assigner and can vary
		depending on the difficulty or length of the task.
	\item
		\textbf{Products and/or services} - The recent introduction of 
		``crowdfunding''~\cite{Howe2008,Belleflamme2011,Schwienbacher2010} platforms 
		such as Kickstarter~\footnote{http://www.kickstarter.com} allow the
		outsourcing of money contributions instead of tasks in exchange for an 
		end product or service that is created utilizing the monetary
		contributions. Crowdfunding is less about utilizing human intelligence
		for computation tasks and more about harnessing the power of a particular
		community to fund and connect them to services or products.
\end{itemize}

An impressive example of how motivating a value incentive can be is
the 2009 Red Balloon Challenge. Teams competed for a \$40,000 prize to find 
10 weather balloons deployed at undisclosed locations across the continental 
United States. The only team to find all balloons accomplished the task in the 
span of 8 hours with crowdsourcing and a unique monetary incentivisation method. 
Their hierarchal incentivisation divided the prize money with participants who 
found the balloons~\cite{Tang2011}, creating a motivation to discover the balloons 
and share the task with as many participants as possible. This method quickly 
motivated thousands of participants to search for the balloon and disseminate
the task on social media sites with the hopes of claiming a piece of the prize.
However, due to the public nature and competitiveness of the task, the team 
experienced plenty of potentially inaccurate and fraudulent submissions. This
negative outcome demonstrates that while value incentives map quickly motivate 
a large number of participants, it is important to note that the quality of 
the data received does not increase with the value offered~\cite{Mason2010a}
and constructs must be placed to ensure the highest quality of collected data.

\subsubsection*{The Greater Good}

In contrast to participants in \textit{Value Incentive} category, 
participants who are engaged and motivated to contribute to a platform 
incentivized by only the idea of pushing forward humanity can be seen 
as ``citizen scientists'', serving \textit{The Greater Good}. These 
citizen scientists often act as collaborators with the task assigners,
volunteering their time to map streets around the world~\cite{Haklay}, 
classify galaxies~\cite{Lintott2008}, translate ancient 
texts~\footnote{http://www.ancientslives.org}, and even assess satellite 
imagery to aid emergency disaster response~\cite{Barrington2012,Goodchild2010}.

The earliest forms of citizen scientists can be traced back to the early 1900s 
with the Christmas Bird count run by the National Audubon 
Society~\cite{Silvertown2009}, where amateur birdwatchers contributed a major 
source of data used to determine the status of bird species in North 
America~\cite{Butcher1990}. More recently, the powerful connectivity of the 
internet has lead to larger scale citizen science projects contributing 
useful data to a number of fields. These collaborations between citizens 
and scientists have shown enormous potential~\cite{Newman2012} as a enhanced
method of collecting and interpreting scientific data.

\subsubsection*{Games With A Purpose}

Games With A Purpose (GWAPs)~\cite{VonAhn2008} attempts to engage individuals
through personal enjoyment or social reward by creating a game environment
in which crowdsourced tasks take place. GWAPs are often related to the 
process of ``gamification'', wherein game elements are added to what may be 
a mundane task, such as image annotation~\cite{VonAhn2004}, in order to motivate 
and engage participants in the task.

While there exist GWAPs where participants may contribute solely for their own 
entertainment value, such as those built around image 
annotation~\cite{VonAhn2004,Ahn2007a,Ahn2006} and collecting ``common-sense 
facts''~\cite{VonAhn2006}, GWAPs can be and often are coupled with the 
previously mentioned \textit{Value Incentive} and \textit{The Greater Good} 
methodology as a means for additional motivation and engagement. 

A recent GWAP, Foldit~\cite{Cooper2010}, created a crowdsourced effort to 
produce accurate protein structure models, successfully using this engagement
and motivation model to attract players who were not only interested in their 
potential contributions to a scientific field but the interaction with other competitive
players in a game environment as well. In cultural heritage, \textit{Exploration:
Mongolia}~\footnote{http://exploration.nationalgeographic.com} uses this approach
to motivate ``armchair archaeologists'' to annotate satellite imagery to aid
the search the Tomb of Genghis Khan in a game environment.

\subsection{Quality Control}
\label{sec:quality-control}

A major issue often discussed in crowdsourcing research is the interesting 
data quality control problems that occur when accepting potentially 
inaccurate or even fraudulent data from human participants. Verifying the
quality of every submitted data point without existing automated methods can 
be as expensive, if not more, as the process of collecting the initial 
data~\cite{Ipeirotis2010a}. As such, researchers have used a variety of 
methods to ensure a higher quality of output from participants which
we explore below.


\subsubsection*{Automatic Verification}

Some crowdsourcing efforts like Foldit~\cite{Cooper2010} have difficult
solutions but involve problems that can be easily verified. Automatic
verification can be used to check the quality of solutions as soon
as a task is completed, often times awarding the participant who
found the solution.


\subsubsection*{Agreement}
Perhaps one of the first methods of quality control, different examples of 
agreement can be seen in the ESP game~\cite{VonAhn2004} and 
TagATune~\cite{Law2009a}. Quality control with agreement accepts input 
if \textit{all} participants agree on the same solution for a task.
The number of participants required to finally accept a solution is
a minimum of two and can vary according to the task.

Agreement can occur before or after a task, respectively
known as input agreement and output agreement. Input agreement, utilized in 
TagATune, involves giving independent participants inputs that may or 
may not be the same and asked to describe the inputs to each other to 
determine the solution to the task. Contrastingly, output agreement, 
utilized in the ESP game, accepts a solution only after two or more 
participants agree on the same solution.


\subsubsection*{Redundancy}
Building upon agreement, redundancy as a form of quality control requires 
multiple participants to complete the same task~\cite{Sheng2008} selecting
a final solution through majority voting or averaging aggregate solutions.
Efforts such as reCAPTCHA~\cite{VonAhn2008} have successfully utilized this 
method to ensure high quality output, at the cost of additional crowdsourcing
work.


\subsubsection*{Reputation}
In reputation systems, a participant's quality of work is determined through 
previous inputs. The quality of a given past input can be determined by the 
task assigner or reviewed by other participants.

Reputation can be used to immediately filter out participants who have a 
high potential of inaccurate input. Amazon Mechanical Turk employs
reputation systems to allow task assigners to specify a minimum reputation 
level for any HITs that are created, thereby ensuring a certain level
of quality in the data that is collected.


\subsubsection*{Expert/Crowdsourced Review}
Quality control using expert or crowdsourced review involve additional
sets of participants who review and check solutions for accuracy during 
or after the initial tasks are completed.

An example of crowdsourced review, Soylent~\cite{Bernstein2010} employs 
this method as the \textit{Find-Fix-Verify} pattern, splitting tasks 
into three stages that filter out inaccurate solutions at each stage 
through redundancy. Monetary rewards, such as those offered for
HITs on Amazon Mechanical Turk, can be withheld until a solution
is reviewed and accepted.


\subsubsection*{Statistical Filtering}
If the solution for a task is expected to match some distribution, 
such as in Quality of Experience (QoE) evaluations~\cite{Chen2009}, researchers 
can filter out inaccurate data points. QoE indicates the degree of a user's 
subjective satisfaction with a particular piece of media, a task 
that can be easily crowdsourced to a large number of participants. By using
paired comparisons, wherein a participant is asked to compare two 
stimuli of varying degrees of quality simultaneously, the results of the task 
can be verified and filtered through simple transitivity.


\subsubsection*{Annotation Models}

Modeling annotation processes~\cite{Karger,Welinder} using a set of variables 
provide a rich method of quality control through the representation of 
different factors that may lead to inaccurate data. These models can 
group participants of certain skill levels together and allow researchers to
filter or combine participants to improve accuracy.

For instance, Welinder et.\ al~\cite{Welinder} look at a set of variables
(competency, expertise, and bias) to represent the annotation ability of
participants and the difficulty of the task, which in this case was
annotating an image. The idea is that competent participants tend to provide
accurate annotations while less competent participants provided 
annotations with inconsistent accuracy. Participants may also have 
certain ``strengths'' or levels of expertise that make certain solutions
easier for them than others. Finally, the difficulty of certain images
may lead to more inaccuracies even by the more competent participants.
Through a Bayesian generative probabilistic model of the annotation process,
Welinder et.\ al found that they could infer factors such as the difficulty
of the image and participant competency and expertise.


Karger et.\ al~\cite{Karger} takes this even further and models both tasks 
and participants to determine an optimal task assignment. Through comparing 
participants' solutions to the same task, we can infer a participant's 
reliability and weigh their answers accordingly. This comparison is done
by constructing a bipartite graph where each edge corresponds to a 
task-participant assignment. An iterative algorithm updates the 
reliability of a participant through each task, ending with a final
estimate of the weighted sum of solutions using the reliability weights
calculated.


\subsection{Discussion}

In this section, we examined how active crowdsourcing requires a large 
distributed network of participants and different models of engagement 
and motivation to successfully complete tasks. Generally,
a combination of different engagement methods are used to attract
the largest number of participants. Furthermore, it is important to note 
that while financial incentives have the ability to motivate large number 
of participants and are utilized as the business models for platforms 
such as Amazon Mechanical Turk, it was found that greater financial 
reward does not necessitate greater motivation and quality from 
the participants~\cite{Mason2010a}.

Once the task is scaled up to enough participants, we arrive at 
the question of data quality and how to ensure the highest level of output
when receiving data from participants with differing levels of experience and 
knowledge. The quality control methodologies discussed have evolved from simple 
agreement~\cite{Law2009a,VonAhn2004} and majority 
voting~\cite{Sheng2008,VonAhn2008} methods which require very little 
post-processing to the more state-of-the-art methods of annotation 
models~\cite{Karger,Welinder} that can represent participants and 
tasks as a set of variables to optimize. Depending on the tasks, simpler
methods may be more attractive than the more involved methods of
annotation modeling. While simpler methods of quality 
control are easier to implement and administrate, many require additional 
costs~\cite{Sheng2008}. These costs are typically from additional tasks for 
participants or experts to verify and accept solutions.


\section{Passive Crowdsourcing}
\label{sec:passive}

Crowdsourcing often invokes the image of large groups of participants in 
front of their computers independently working on a task. Passive 
crowdsourcing, ``work for nothing''~\cite{Adar2011}, relies on being able 
to gather data or perform computations as a result of the existing behavior 
of a group of participants with or without their knowledge. Due to its 
passive, often hidden nature, passive crowdsourcing bypasses many of the 
problems due to crowd motivation and engagement present in active 
crowdsourcing techniques, while also raising questions about privacy
and the ethics of gathering data without a participant's knowledge.


\subsection{Parasitic Computing~\cite{Barabasi2001}}
\label{sec:parasitic-computing}
First coined by Albert-Laszlo Barabasi in 2001, parasitic computing 
leverages existing behavior of individuals to solve pieces of a complex 
computation problem, often without their explicit permission. This 
has involved harnessing small amounts of a participant's computational 
power~\cite{Anderson2002,Merelo2007,Merelo-Guervos2008} or by 
embedding the computations into an underlying standard 
protocol~\cite{Barabasi2001,Kohring2003}.

Initially, parasitic computing utilized underlying standard protocols
as a layer in which to do small pieces of computational work. For 
example, Barabasi~\cite{Barabasi2001} utilized the Hyper Text 
Transmission Protocol (HTTP) to perform calculations to solve
a NP-complete satisfiability problem. The calculations were 
accomplished using a standard characteristic of TCP, a checksum which
is checked by a receiving computer and can be formulated such that
the receiving computer performs any arithmetic operation. A special
message is created to represent a potential solution to an NP-complete
problem, such as 3-SAT, and sent to many targets. Since TCP messages
with an incorrect checksum are dropped, only those that contained 
a valid solution will receive a reply. 

Kohring~\cite{Kohring2003} further explored this concept, embedding the 
computations as part of the standard IP communication protocol through 
the use of carefully constructed ICMP packets. The use of IP and ICMP
increases the overall target surface as the targets are no longer
restricted to HTTP servers like Barabasi implementation and any
Internet capable device is required to implement ICMP. Much like 
Barabasi, Kohring utilized a checksum in the ICMP message to encode
the calculation needed. However, since ICMP does not contain a guarantee
that a message will arrive in order or even arrive at all, the
problems that can be solved are more restricted. In this case, 
Kohring simulated a stochastic neural network due to its built-in 
resilience to noise.

More recent approaches harness idle or underutilized CPUs to do
pieces of computational work. Acting as a screensaver for idle 
computers SETI@home~\cite{Anderson2002} has been able to harness 
millions of computers around the world to analyze radio signals 
from space with the explicit permission of participants. Merelo 
et al.~\cite{Merelo2007,Merelo-Guervos2008} has explored this 
concept as a web application by running genetic algorithms in web 
browsers using a small percentage of computation resources as the 
result of web page being loaded.


\subsection{Crowdsensing}
\label{sec:crowdsensing}
In contrast to parasitic computing, crowdsensing~\cite{Ganti2011} 
involves the collection of data from participants through existing 
behavior as a method of providing a solution to those same participants.
This often involves, but is not limited to, embedded sensors on 
consumer-centric mobile devices or other computing devices with wireless 
capabilities that produce data as a participant interacts with the device 
or world through their existing behavior.

A remarkable example of crowdsensing without the use of mobile sensors is
Google Flu Trends (GFT)~\footnote{http://www.google.org/flutrends}. 
GFT uses search query data for influenza related search terms provided 
by millions of Google users to estimate influenza activity. These 
results have been shown as an accurate predictor of influenza 
trends~\cite{Dugas2012} tracking influenza even before 
major national disease centers.

Building upon the existing need and activity of checking
traffic, Google Traffic~\cite{GoogleTraffic} and VTrack~\cite{Thiagarajan2009}
crowd sense real-time traffic information through the act of
using their map or traffic checking application.

A non-intuitive, yet powerful example of crowdsensing relates
to 3D reconstruction from photographs~\cite{Agarwal2009,Frahm2010},
which requires large amounts of imagery, has benefitted from
crowdsensing through the use of publicly available photographs
on websites such as Flickr~\footnote{http://www.flickr.com}. This 
example is especially useful in digital preservation of cultural
heritage, where publicly available photos, such as those taken 
by tourists, of important cultural heritage sites could be used 
to construct models.

\subsection{Discussion}

In this section, we explored the concept of passive crowdsourcing
which relies on the ability to gather data and/or perform computations
as a result of existing participant behavior with or without their
knowledge. The rise of embedded sensors in everyday computing
devices such as mobile devices and the ever expanding network
of Internet enable devices have created a largely untapped potential
source of data and computational power. Parasitic computing taps
into existing Internet infrastructure to perform computations, 
such as using HTTP or ICMP checksums to harness the computation power of 
other computers. Crowdsensing looks to using embedded sensors 
available in everyday computing devices to capture useful information
present in the existing behavior of participants, such as traffic
or flu information.

However due to the opportunistic nature of passive crowdsourcing, 
especially in crowdsensing, sensitive data pertaining to individuals must
be kept private and secure while also ensuring passive crowdsourcing
applications still provide some level of utility. Some simple procedures,
such as running analytics on the aggregate~\cite{Ganti2011} of all 
data and only keeping data that is anonymous and unattached to any 
single participant goes a long way to protecting the identity of the 
participants on the platform. Alternatively, differential 
privacy~\cite{Dwork2006} methods could be used to encrypt 
sensitive data while still provided statistically accurate
analysis.

\section{Learning from the Crowd}
\label{sec:learning}
Crowdsourcing has the potential to collect an enormous amount of
information, all of which can be harnessed to partially or fully 
automate the tasks from which the data originated through the use of 
machine learning. Machine learning approaches excel at recognizing
consistent patterns among labeled training examples, such as those
provided by crowdsourcing participants, which can then be used to 
label new, unlabeled data.

Pattern recognition techniques rely on the extraction of features
that describe the properties of the concept being recognized. For
example, feature extraction on images can refer to edges, corners,
shapes, or even color distribution. Machine learning algorithms
can then identify statistical consistencies between sets of
features and can apply this knowledge to future sets of new,
unlabeled data.

While traditional machine learning approaches focus on a single,
fixed training set to learn static models, crowdsourced data presents
a source of temporally dynamic data which can be used to continually
create new, improved models. This methodology, known as active 
learning~\cite{Settles2010,Druck2009}, is able to query some outside
source for additional information that can be used to improve a model.
Generally, the additional information comes in the form of additional
labels from participants that can be applied to instances with no
clear classification. Active learning has been shown to produce reliable,
accurate models~\cite{Barrington2012,Brew2010}.


\subsection{Active Learning}
Active learning techniques can be categorized into two overarching 
methodologies based on how additional information is acquired to 
improve the underlying model. 

\textit{Stream-based}~\cite{Freund1997} or \textit{online} sampling 
approaches require a decision as each unlabeled instance is queried. The key
assumption in online active learning is that obtaining an unlabeled instance
is free or inexpensive so it can be sampled from a source one at a time. Due to 
the sequential querying nature of stream-based approaches, applications of 
stream-based approaches must decide of whether or not to continue querying for 
instances. This decision can rely on some ``informative measure'' or ``query 
strategy'' such that more informative instances are more likely to be 
queried~\cite{Settles2010}.

In contrast, \textit{pool-based}~\cite{McCallum1998} sampling approaches allow an algorithm to choose from a typically large set of unlabeled data based on some quantitative measure and a small set of labeled data. For many real-world learning problems, a large set of unlabeled data can be gathered at once, for instance using crowdsourcing. Pool-based methods are typically studied in more detail, used in approaches stemming from text classification~\cite{Lewis1994,Yan2011}, natural language processing~\cite{Snow2008}, and more.

Furthermore, we can categorized active learning algorithms based on the types of 
training data available~\cite{Barrington2012}. \textit{Discriminative} methods 
harness both positive and negative training data to create a model. In contrast, 
\textit{generative} methods require only positive training data while negative 
training data does not increase the accuracy of the model. 

Active learning presents a simple, powerful approach to harness 
crowdsourced human annotators in collaboration with machine learning 
techniques to continually formulate new, improved models as more and more 
data is collected.


\subsection{Closing the loop}
Querying new training instances through uncertainty 
sampling~\cite{Lewis1994} involves the selection of instances that have low 
classification or prediction confidence. Using this selection criteria, we 
can have new human annotators label instances where we have low confidence, 
re-train our model, and repeat the process thus completing the loop between 
machine learning and human annotators.

While uncertainty sampling provides us with a useful criterion for 
selecting unlabeled instances, it does not provide a method to select a 
annotator. Annotation modeling methods discussed in 
Section~\ref{sec:quality-control} can potentially be used to pair new 
unlabeled instance with an optimal annotator. Karger et. al~\cite{Karger} 
used bipartite graphs to model tasks and annotators. Yan et.\ 
al~\cite{Yan2011} presents an optimization problem to select the best 
possible annotator based on the current iteration of data and model.

Due to the fact that human annotations are inherently noisy, an open 
research question~\cite{Settles2010} is the decision of whether to label a 
new unlabeled instance or repeat the labeling of an existing instance to 
potentially improve the model. Research~\cite{Sheng2008} using heuristics 
representing uncertainty in both the human annotators and the active 
learning model have shown that repeated labeling improves the overall data 
collected but assumes that all human annotators represent the same level of 
expertise and that there exists some true label for any instance. 
Addressing the first assumption, Donmez et.\ al~\cite{Donmez2009} allowed 
human annotators to have different ``noise'' levels showing that 
both true labels and the expert level of individual human annotators can be 
estimated, thus allowing them to query only the more expert human 
annotators in later active learning iterations.


\subsection{Discussion}
By itself, crowdsourcing and human computation tasks are powerful but 
difficult to scale without constantly active engagement and motivation. This
section explored the utility of using crowdsourcing to gather a small, but still
substantial set of annotated data to be used for training machine learning
algorithms. Additionally, if we come across data that can not be classified by 
our model we can further employ the concept of active learning and uncertainty
sampling to close the loop where data goes through humans $\Rightarrow$ active 
learning $\Rightarrow$ humans. Active learning coupled with inexpensive 
crowdsourced data has been shown to be a cost effective method that is as 
accurate as more expensive expert-based machine 
learning~\cite{Barrington2012,Yan2011}, requiring only a small number of non-
expert participants to match the performance of an expert 
participant~\cite{Snow2008}.


\section{Applications in Cultural Heritage}
\label{sec:applications-heritage}
While the majority of crowdsourcing techniques have thus far focused on 
fields outside of cultural heritage, the massive data collection 
capabilities of the crowd have started to find ways to improve the 
interpretation, digital preservation, and exploration/discovery of cultural 
heritage objects and sites of interest.

More importantly, the engagement of the public with the interpretation, 
preservation, and discovery of cultural heritage offers an opportunity for
a participant to do something more than simply consume information, ``engag[ing] 
them in the fundamental reason that these digital collections exist in the 
first place~\cite{Owens2013}.''


\subsection{Interpretation}
In museums, collections of cultural heritage objects are traditionally organized
into galleries based around a common theme by professional curators. This
organization helps museum visitors interpret the collections~\cite{Aletras2012}
For digital collections, there may not be an clear organization or theme making
large collections of digital collections overwhelming. Crowdsourcing different
interpretations of a set of cultural heritage creates a compelling human 
computation task to creatively link together related pieces of human history.

BBC \textit{WW2 People's War}
\footnote{http://www.bbc.co.uk/history/ww2peopleswar} was a platform
where the public was allowed to submit ``memories'' of World War 2 collecting 
over 47,000 stories and 14,000 images of how an entire generation 
interpreted the war and preserving it for future generations. This platform
provided a unique opportunity for a generation of people to provide their 
own interpretations and get in touch with others we shared the same 
experiences.

The datasets that make up large digital collections make curation by the crowd 
a viable solution albeit one that has not fully been explored for the cultural
heritage domain.


\subsection{Digital Preservation}
Perhaps the earliest example of using crowdsourcing for cultural heritage was
the use of CAPTCHAs~\cite{Ahn2003}, an automated test to tell humans and computers apart, to transcribe and digitize old texts. CAPTCHAs typically 
consist of an image containing random distorted characters, used as a challenge
response during Web form registrations or submissions to prove that the 
submitter is human. The distorted characters present a significantly 
difficult problem for computers or ``bots'' that attempt to automatically submit Web forms, while only a minor inconvenience for humans.

Foundations and projects such as the Google Books Project have been scanned 
in an attempt to preserve human knowledge and make it more accessible. While
optical character recognition (OCR) technology can transcribe a large majority
of the imagery scanned, there  exists a significant chunk 
(20\%~\cite{VonAhn2008a} of all scanned text) that require human transcription,
who tend to be expensive. Through the combination of CAPTCHAs and the scanned 
imagery from text digitization projects, reCAPTCHAs~\cite{VonAhn2008a} replaces 
the random distorted characters used in CAPTCHAs with words that OCR programs
can not recognize. reCAPTHCAs have enabled historical data that have no
digital equivalent to now be fully preserved and accessible for millions of
humans across the world.

Perhaps a more meaningful example that demonstrates the impact of 
crowdsourcing cultural heritage preservation, the University of Iowa 
libraries crowdsourced the transcription of a set of Civil War 
diaries~\cite{Owens2013}. As a surprising byproduct of the transcription, the
project dramatically increased the number of visitors to the library's
website and was shown to be a novel meaningful engagement between the human
annotators and this piece of history. These connections between the participants and the content allow them to engage with the collections and with the past, the ``apex of user experience for cultural heritage collections''~\cite{Owens2013}, while offering the useful byproduct of transcription.


\subsection{Exploration \& Discovery}
New cultural heritage sites are still being unearthed and 
analyzed~\cite{Lasaponara2007,Rowlands2007,Alexakis2009}, increasingly 
through more modern methods and often utilizing large amounts of
high resolution satellite and/or aerial imagery. As the resolution of
these images increase with improved technology, the amount of data that must
be sifted through to discover new sites increases significantly. Interpreting
ambiguous features in the satellite imagery also present additional 
difficulties, all of which can be crowdsourcing as human computation tasks.

The \textit{Valley of the Khans} project opens this exploration and discovery
process to the public through Exploration: 
Mongolia~\footnote{http://exploration.nationalgeographic.com} an
an online platform motivating human annotators to survey high-resolution
satellite imagery in the hopes of discovering the lost tomb of Genghis Khan.
Annotators are asked to categorize roads, rivers, modern structures, and
ancient structures in the satellite imagery to assist field efforts in
navigating the large study area. During three field surveys, over 100
sites of interest were visited, resulting in the discovery of fifty-five
archaeological sites, ranging a human existence of over 3000 years.


\subsection{Discussion}
As explored in this section, while crowdsourcing is often though of as an 
instrument for data collection, it has shown the potential to be so much more. 
Cultural heritage exists to connect people today with the knowledge and 
experiences of the past and crowdsourcing gives researchers and curators of 
cultural heritage collections the ability to reach out to new participants while 
also as a byproduct enriching their own datasets.

Furthermore, as more and more collections turn to digital techniques for 
preservation and analysis, there will an increasingly amount of data and tasks 
that can benefit from human computation and crowdsourcing. While still being 
explored, the crowdsensing techniques described to create 3D models from 
photographs~\cite{Agarwal2009} could someday be used to accurately preserve 
and analyze historic structures and objects. Crowdsourced methods of exploration 
and discovery of new sites of interest with high-resolution satellite imagery
are also being researched.


\section{Towards the Future}

Crowdsourcing has shown increasing potential as a platform to perform
human computation tasks and its many successes across a number of different 
fields have shown that serious work can be done using inexpensive, distributed 
participants. We found (Section~\ref{sec:active}) that crowd engagement was a 
large part of the successes in different crowdsourcing platforms, and depending 
on the type of task, there are multiple methods that can be used to motivate 
participants to finish tasks.

Furthermore, the use of passive crowdsourcing (Section~\ref{sec:passive}) has 
side-stepped the issues of motivation and crowd engagement that may plague 
active crowdsourcing platforms. Passive crowdsourcing is increasingly being used 
in novel ways that can measure and collect data from existing participant 
behavior. While limited in the number of potential applications, passive 
crowdsourcing provides a powerful alternative to traditional crowdsourcing 
platforms as a means of data collection and distributed computation.

A potential solution and progression in crowdsourced data analytics is the use 
of machine learning algorithms (Section~\ref{sec:learning}) to search for 
patterns existing within the collected data. This reduces the number of 
potential participants and tasks, reducing the financial burden while also 
providing a way to scale past the limits of crowdsourcing. As a way to 
continually increase the accuracy of machine learning models, we can use the 
concept of active learning which allows us to iteratively improve a model with 
additional information.

While crowdsourcing is often thought of as an instrument for data collection, it 
has shown the potential to be so much more~\cite{Owens2013}. As discussed in 
Section~\ref{sec:applications-heritage}, as digitization methodology improves 
over the years, there will an ever increasing demand for more and more human 
computation to analyze, interpret, and discovery digital cultural heritage. 
Cultural heritage has the unique and powerful feature of involving the beneficiaries of the heritage being preserved or analyzed in the process of the actual preservation and analysis. A ``crowdsourcing'' platform for cultural heritage does not only have to serve as a platform for distributing tasks and collecting solutions but can also serve as a platform for connecting the participants to the history involved.

\section{Acknowledgements}
Thanks to Professor Yoav Freund for chairing my research exam committee and 
for Serge Belongie and Bill Griswold for participating on the committee. Thanks
for Albert Lin for proofreading and giving feedback on drafts.

\bibliography{bibs/ActiveLearning,bibs/Andrew-HCOMP,bibs/Andrew-CogSci,bibs/CulturalHeritage,bibs/hcomp,bibs/Web,bibs/Misc}
\bibliographystyle{plain}

\end{document}
